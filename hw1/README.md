# Описание

Утро понедельника. Вы только открыли ноут, и тут к вам врывается менеджер Марк из маркетинга (вы удивленно смотрите на него, поскольку работаете на удаленке) и заявляет, что в ваших руках судьба компании!

Для продвижения продуктов срочно требуется закупить рекламу на Youtube. Но делать это вслепую - слишком дорогое удовольствие. Поэтому нужно провести скоринг трендовых видео на Youtube и предоставить данные аналитикам для поиска наиболее перспективной стратегии.

Хорошо, что Марк уже нашел данные - это датасет https://www.kaggle.com/datasets/datasnaek/youtube, откуда нужно будет взять данные по американскому рынку (3 файла, начинающиеся с US).

Парсер комментариев работает не очень, поэтому, если какие-то записи в файле с комментариями окажутся битыми - можно их пропустить.

Марк очень заинтересован в задаче и поэтому уже подготовил (а скорее всего уговорил кого-то из коллег подготовить) репозиторий для вашей работы - https://github.com/Gorini4/pyspark_course_excercises/tree/main/hw1 

В итоге марку нужны следующие датасеты:
1. scored_videos - датасет на основе файла USvideos.csv с добавлением колонки, содержащей скор (показатель качества) видео. Никто не знает, как считать скор, поэтому формулу предлагается придумать вам. Но она должна включать в себя просмотры, лайки, дизлайки видео, лайки и дизлайки к комментариям к этому видео.
1. categories_score - датасет по категориям, в котором присутствуют следующие поля:
Название категории (не id, он непонятный для аналитиков!).
Медиана показателя score из датасета scored_videos по каждой категории.
1. popular_tags - датасет по самым популярным тэгам (название тэга + количество видео с этим тэгом). В исходном датасете тэги лежат строкой в поле tags. Другие разработчики уже сталкивались с подобной задачей, поэтому написали Scala-функцию для разбиения тегов (код функции - в разделе Примечания). Но не доверяйте им вслепую! Обязательно напишите свою UDF-функцию разбиения строки на тэги и сравните время работы с её Scala-версией. Можно замерять своими силами, а можно воспользоваться библиотекой timeit. Стандартные функции Spark из пакета `pyspark.sq.functions` использовать нельзя, нужно написать свою функцию.
1. И личная просьба от Марка: он любит котов (а кто не их не любит!) и хочет найти самые интересные комментарии (топ-5) к видео про котов. “Видео про котов” - видео, у которого есть тэг “cat”.

# Критерии выполнения

1. В качестве результата должна быть выслана ссылка на публичный git-репозиторий, созданный на основе репозитория, указанного в описании.
1. Код решения должен быть в notebook Video Scoring.ipynb (уже находится в репозитории-примере).
1. Все итоговые датасеты должны быть отображены в ноутбуке через .show() .
1. Везде, где это возможно, обычные UDF должны быть заменены на Pandas UDF или Pandas API on Spark.
1. Датасеты USvideos.csv и UScomments.csv считаем огромными (нельзя помещать в broadcast).
1. Для каждого join должна быть использована какая-либо из оптимизаций, предложенных на лекции. Выбор типа оптимизации нужно обосновать, описав его в комментариях в ноутбуке.
1. Для расчета медианы нельзя использовать встроенную Spark-функцию median из пакета pyspark.sql.functions .
1. Для функции разделения тегов должны быть представлены выводы по сравнению времени работы обычной и scala-версии.

# Примечания

- Для выполнения этого ДЗ вам потребуется компьютер с установленным Docker’ом - https://docs.docker.com/engine/install/ 
- Для запуска окружения выполните команду “docker compose up” из директории hw1 и перейдите по ссылке из логов (ссылка вида http://127.0.0.1:8888/?token=xxxxxxxxxxxxxxxxxxxxxxxxxx).
- Для оптимизации Join можно использовать не только алгоритмы, но и предварительную подготовку датасетов (бакетинг, партицирование).
- Если возникнут проблемы, не стесняйтесь задавать вопросы в чате - наверняка, с этими же проблемами столкнется кто-то еще из ваших товарищей.
- Код Scala-функции 
```scala
object CustomUDFs {
  def splitTags(s: String): Array[String] = s.split('|')
  val splitTagsUDF: UserDefinedFunction = udf(splitTags _)
}
```

